{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/omniverse/workspace/robomimic\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import h5py\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import robomimic\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "from robomimic.envs.env_base import EnvBase\n",
    "from robomimic.algo import RolloutPolicy\n",
    "\n",
    "import urllib.request\n",
    "%cd /home/omniverse/workspace/robomimic/\n",
    "\n",
    "# print LD_LIBRARY_PATH\n",
    "import os\n",
    "# print(os.environ['LD_LIBRARY_PATH'])\n",
    "# print(list(os.environ))\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-12.1/lib64:/home/omniverse/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/omniverse/workspace/PyRep/CoppeliaSim_Edu_V4_1_0_Ubuntu20_04'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(results_paths, models_path):\n",
    "    success_rate = np.load(os.path.join(results_paths, \"agent_results\", \"success_rates.npy\"))\n",
    "    # get idx of the best model\n",
    "    best_model_idx = np.argmax(success_rate.mean(axis=1))\n",
    "    plt.figure()\n",
    "    plt.plot(success_rate.mean(axis=1))\n",
    "    plt.show()\n",
    "    print(\"Best model idx: \", best_model_idx)\n",
    "    print(\"Best model success rate: \", success_rate[best_model_idx].mean())\n",
    "    print(\"Length of models_path: \", len(models_path))\n",
    "    return models_path[best_model_idx]\n",
    "\n",
    "def get_best_model_idx(results_paths):\n",
    "    success_rate = np.load(os.path.join(results_paths, \"agent_results\", \"success_rates.npy\"))\n",
    "    # get idx of the best model\n",
    "    best_model_idx = np.argmax(success_rate.mean(axis=1))\n",
    "    return best_model_idx\n",
    "\n",
    "\n",
    "def rollout(policy, env, horizon, render=False, video_writer=None, video_skip=5, camera_names=None):\n",
    "    \"\"\"\n",
    "    Helper function to carry out rollouts. Supports on-screen rendering, off-screen rendering to a video, \n",
    "    and returns the rollout trajectory.\n",
    "    Args:\n",
    "        policy (instance of RolloutPolicy): policy loaded from a checkpoint\n",
    "        env (instance of EnvBase): env loaded from a checkpoint or demonstration metadata\n",
    "        horizon (int): maximum horizon for the rollout\n",
    "        render (bool): whether to render rollout on-screen\n",
    "        video_writer (imageio writer): if provided, use to write rollout to video\n",
    "        video_skip (int): how often to write video frames\n",
    "        camera_names (list): determines which camera(s) are used for rendering. Pass more than\n",
    "            one to output a video with multiple camera views concatenated horizontally.\n",
    "    Returns:\n",
    "        stats (dict): some statistics for the rollout - such as return, horizon, and task success\n",
    "    \"\"\"\n",
    "    assert isinstance(env, EnvBase)\n",
    "    assert isinstance(policy, RolloutPolicy)\n",
    "    assert not (render and (video_writer is not None))\n",
    "\n",
    "    policy.start_episode()\n",
    "    obs = env.reset()\n",
    "    state_dict = env.get_state()\n",
    "\n",
    "    # hack that is necessary for robosuite tasks for deterministic action playback\n",
    "    obs = env.reset_to(state_dict)\n",
    "\n",
    "    results = {}\n",
    "    video_count = 0  # video frame counter\n",
    "    total_reward = 0.\n",
    "    try:\n",
    "        for step_i in range(horizon):\n",
    "\n",
    "            # get action from policy\n",
    "            act = policy(ob=obs)\n",
    "\n",
    "            # play action\n",
    "            next_obs, r, done, _ = env.step(act)\n",
    "\n",
    "            # compute reward\n",
    "            total_reward += r\n",
    "            success = env.is_success()[\"task\"]\n",
    "\n",
    "            # visualization\n",
    "            if render:\n",
    "                env.render(mode=\"human\", camera_name=camera_names[0])\n",
    "            if video_writer is not None:\n",
    "                if video_count % video_skip == 0:\n",
    "                    video_img = []\n",
    "                    for cam_name in camera_names:\n",
    "                        video_img.append(env.render(mode=\"rgb_array\", height=512, width=512, camera_name=cam_name))\n",
    "                    video_img = np.concatenate(video_img, axis=1) # concatenate horizontally\n",
    "                    video_writer.append_data(video_img)\n",
    "                video_count += 1\n",
    "\n",
    "            # break if done or if success\n",
    "            if done or success:\n",
    "                break\n",
    "\n",
    "            # update for next iter\n",
    "            obs = deepcopy(next_obs)\n",
    "            state_dict = env.get_state()\n",
    "\n",
    "    except env.rollout_exceptions as e:\n",
    "        print(\"WARNING: got rollout exception {}\".format(e))\n",
    "\n",
    "    stats = dict(Return=total_reward, Horizon=(step_i + 1), Success_Rate=float(success))\n",
    "\n",
    "    return stats\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster_output/mrl_trained_models_ds/mrl_square_ph/20240913173310', 'cluster_output/mrl_trained_models_ds/mrl_square_ph/agent_results']\n",
      "cluster_output/mrl_trained_models_ds/mrl_square_ph/20240913173310/models/model_epoch_200.pth\n",
      "============= Loaded Config =============\n",
      "{\n",
      "    \"algo_name\": \"mrl\",\n",
      "    \"experiment\": {\n",
      "        \"name\": \"mrl_square_ph\",\n",
      "        \"validate\": false,\n",
      "        \"logging\": {\n",
      "            \"terminal_output_to_txt\": true,\n",
      "            \"log_tb\": false,\n",
      "            \"log_wandb\": true,\n",
      "            \"wandb_proj_name\": \"MetricRL_Baselines\"\n",
      "        },\n",
      "        \"save\": {\n",
      "            \"enabled\": true,\n",
      "            \"every_n_seconds\": null,\n",
      "            \"every_n_epochs\": 50,\n",
      "            \"epochs\": [],\n",
      "            \"on_best_validation\": false,\n",
      "            \"on_best_rollout_return\": false,\n",
      "            \"on_best_rollout_success_rate\": true\n",
      "        },\n",
      "        \"epoch_every_n_steps\": 100,\n",
      "        \"validation_epoch_every_n_steps\": 10,\n",
      "        \"env\": null,\n",
      "        \"additional_envs\": null,\n",
      "        \"render\": false,\n",
      "        \"render_video\": true,\n",
      "        \"keep_all_videos\": false,\n",
      "        \"video_skip\": 5,\n",
      "        \"rollout\": {\n",
      "            \"enabled\": false,\n",
      "            \"n\": 50,\n",
      "            \"horizon\": 400,\n",
      "            \"rate\": 50,\n",
      "            \"warmstart\": 0,\n",
      "            \"terminate_on_success\": true,\n",
      "            \"goal_path\": \"datasets/square/ph/low_dim_v141_augmented_goal.hdf5\"\n",
      "        }\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"data\": \"datasets/square/ph/low_dim_v141_augmented.hdf5\",\n",
      "        \"output_dir\": \"../output/mrl_trained_models_ds\",\n",
      "        \"num_data_workers\": 0,\n",
      "        \"hdf5_cache_mode\": \"all\",\n",
      "        \"hdf5_use_swmr\": true,\n",
      "        \"hdf5_load_next_obs\": true,\n",
      "        \"hdf5_normalize_obs\": false,\n",
      "        \"hdf5_filter_key\": null,\n",
      "        \"hdf5_validation_filter_key\": null,\n",
      "        \"seq_length\": 1,\n",
      "        \"pad_seq_length\": true,\n",
      "        \"frame_stack\": 1,\n",
      "        \"pad_frame_stack\": true,\n",
      "        \"dataset_keys\": [\n",
      "            \"actions\",\n",
      "            \"rewards\",\n",
      "            \"dones\"\n",
      "        ],\n",
      "        \"goal_mode\": \"custom\",\n",
      "        \"cuda\": true,\n",
      "        \"batch_size\": 1024,\n",
      "        \"num_epochs\": 2000,\n",
      "        \"seed\": 1\n",
      "    },\n",
      "    \"algo\": {\n",
      "        \"phi_dim\": 64,\n",
      "        \"pre_train\": false,\n",
      "        \"pre_train_epochs\": 500,\n",
      "        \"optim_params\": {\n",
      "            \"critic\": {\n",
      "                \"learning_rate\": {\n",
      "                    \"initial\": 0.001,\n",
      "                    \"decay_factor\": 0.0,\n",
      "                    \"epoch_schedule\": []\n",
      "                },\n",
      "                \"regularization\": {\n",
      "                    \"L2\": 0.0\n",
      "                }\n",
      "            },\n",
      "            \"vf\": {\n",
      "                \"learning_rate\": {\n",
      "                    \"initial\": 0.0001,\n",
      "                    \"decay_factor\": 0.0,\n",
      "                    \"epoch_schedule\": []\n",
      "                },\n",
      "                \"regularization\": {\n",
      "                    \"L2\": 0.0\n",
      "                }\n",
      "            },\n",
      "            \"actor\": {\n",
      "                \"learning_rate\": {\n",
      "                    \"initial\": 0.0003,\n",
      "                    \"decay_factor\": 0.0,\n",
      "                    \"epoch_schedule\": []\n",
      "                },\n",
      "                \"regularization\": {\n",
      "                    \"L2\": 0.0\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"discount\": 0.99,\n",
      "        \"target_tau\": 0.005,\n",
      "        \"actor\": {\n",
      "            \"net\": {\n",
      "                \"type\": \"gaussian\",\n",
      "                \"common\": {\n",
      "                    \"std_activation\": \"softplus\",\n",
      "                    \"low_noise_eval\": true,\n",
      "                    \"use_tanh\": false\n",
      "                },\n",
      "                \"gaussian\": {\n",
      "                    \"init_last_fc_weight\": 0.001,\n",
      "                    \"init_std\": 0.3,\n",
      "                    \"fixed_std\": false\n",
      "                },\n",
      "                \"gmm\": {\n",
      "                    \"num_modes\": 5,\n",
      "                    \"min_std\": 0.0001\n",
      "                }\n",
      "            },\n",
      "            \"layer_dims\": [\n",
      "                300,\n",
      "                400\n",
      "            ],\n",
      "            \"max_gradient_norm\": null\n",
      "        },\n",
      "        \"critic\": {\n",
      "            \"ensemble\": {\n",
      "                \"n\": 1\n",
      "            },\n",
      "            \"layer_dims\": [\n",
      "                300,\n",
      "                400\n",
      "            ],\n",
      "            \"use_huber\": false,\n",
      "            \"max_gradient_norm\": null\n",
      "        },\n",
      "        \"adv\": {\n",
      "            \"clip_adv_value\": null,\n",
      "            \"beta\": 1.0,\n",
      "            \"use_final_clip\": true\n",
      "        },\n",
      "        \"vf_quantile\": 0.9\n",
      "    },\n",
      "    \"observation\": {\n",
      "        \"modalities\": {\n",
      "            \"obs\": {\n",
      "                \"low_dim\": [\n",
      "                    \"robot0_eef_pos\",\n",
      "                    \"robot0_eef_quat\",\n",
      "                    \"robot0_gripper_qpos\",\n",
      "                    \"object\"\n",
      "                ],\n",
      "                \"rgb\": [],\n",
      "                \"depth\": [],\n",
      "                \"scan\": []\n",
      "            },\n",
      "            \"goal\": {\n",
      "                \"low_dim\": [],\n",
      "                \"rgb\": [],\n",
      "                \"depth\": [],\n",
      "                \"scan\": []\n",
      "            }\n",
      "        },\n",
      "        \"encoder\": {\n",
      "            \"low_dim\": {\n",
      "                \"core_class\": null,\n",
      "                \"core_kwargs\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            },\n",
      "            \"rgb\": {\n",
      "                \"core_class\": \"VisualCore\",\n",
      "                \"core_kwargs\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            },\n",
      "            \"depth\": {\n",
      "                \"core_class\": \"VisualCore\",\n",
      "                \"core_kwargs\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            },\n",
      "            \"scan\": {\n",
      "                \"core_class\": \"ScanCore\",\n",
      "                \"core_kwargs\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"meta\": {\n",
      "        \"hp_base_config_file\": null,\n",
      "        \"hp_keys\": [],\n",
      "        \"hp_values\": []\n",
      "    }\n",
      "}\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot0_gripper_qpos', 'robot0_eef_quat', 'object', 'robot0_eef_pos']\n",
      "using obs modality: rgb with keys: []\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n",
      "============= Loaded Policy =============\n",
      "ObservationKeyToModalityDict: mean not found, adding mean to mapping with assumed low_dim modality!\n",
      "ObservationKeyToModalityDict: scale not found, adding scale to mapping with assumed low_dim modality!\n",
      "ObservationKeyToModalityDict: value not found, adding value to mapping with assumed low_dim modality!\n",
      "MetricRL (\n",
      "  ModuleDict(\n",
      "    (actor): MRLGaussianActorNetwork(\n",
      "        action_dim=7\n",
      "        fixed_std=False\n",
      "        std_activation=softplus\n",
      "        init_std=0.3\n",
      "        mean_limits=[-9.  9.]\n",
      "        std_limits=[7.0e-03 7.5e+00]\n",
      "        low_noise_eval=True\n",
      "  \n",
      "        encoder=ObservationGroupEncoder(\n",
      "            group=obs\n",
      "            ObservationEncoder(\n",
      "                Key(\n",
      "                    name=object\n",
      "                    shape=[14]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot0_eef_pos\n",
      "                    shape=[3]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot0_eef_quat\n",
      "                    shape=[4]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot0_gripper_qpos\n",
      "                    shape=[2]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                output_shape=[23]\n",
      "            )\n",
      "        )\n",
      "  \n",
      "        mlp=MLP(\n",
      "            input_dim=23\n",
      "            output_dim=400\n",
      "            layer_dims=[300]\n",
      "            layer_func=Linear\n",
      "            dropout=None\n",
      "            act=ReLU\n",
      "            output_act=ReLU\n",
      "        )\n",
      "  \n",
      "        decoder=ObservationDecoder(\n",
      "            Key(\n",
      "                name=mean\n",
      "                shape=(7,)\n",
      "                modality=low_dim\n",
      "                net=(Linear(in_features=400, out_features=7, bias=True))\n",
      "            )\n",
      "            Key(\n",
      "                name=scale\n",
      "                shape=(7,)\n",
      "                modality=low_dim\n",
      "                net=(Linear(in_features=400, out_features=7, bias=True))\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "    (critic): MRLValueNetwork(\n",
      "        value_bounds=None\n",
      "  \n",
      "        encoder=ObservationGroupEncoder(\n",
      "            group=obs\n",
      "            ObservationEncoder(\n",
      "                Key(\n",
      "                    name=object\n",
      "                    shape=[14]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot0_eef_pos\n",
      "                    shape=[3]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot0_eef_quat\n",
      "                    shape=[4]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot0_gripper_qpos\n",
      "                    shape=[2]\n",
      "                    modality=low_dim\n",
      "                    randomizer=None\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                output_shape=[23]\n",
      "            )\n",
      "        )\n",
      "  \n",
      "        mlp=MLP(\n",
      "            input_dim=23\n",
      "            output_dim=400\n",
      "            layer_dims=[300]\n",
      "            layer_func=Linear\n",
      "            dropout=None\n",
      "            act=ReLU\n",
      "            output_act=ReLU\n",
      "        )\n",
      "  \n",
      "        decoder=ObservationDecoder(\n",
      "            Key(\n",
      "                name=value\n",
      "                shape=(64,)\n",
      "                modality=low_dim\n",
      "                net=(Linear(in_features=400, out_features=64, bias=True))\n",
      "            )\n",
      "        )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# results_path = \"../../output/mrl_trained_models_ds/\"\n",
    "# dataset = \"mg\"\n",
    "# task = \"lift\"\n",
    "# results_paths = glob.glob(results_path + \"MetricRL_\" +  task + \"_\" + dataset + \"/*\")[0]\n",
    "\n",
    "results_path = \"cluster_output/\"\n",
    "\n",
    "# datasets = [\"ph\", \"mh\"]#, \"mg\"]\n",
    "# tasks = [\"square\", \"transport\"] # square transport\n",
    "\n",
    "model = \"mrl\"\n",
    "dataset = \"ph\"\n",
    "task = \"square\"\n",
    "\n",
    "results_path = results_path + f\"{model}_trained_models_ds/{model}_{task}_{dataset}\"\n",
    "results_paths = glob.glob(results_path + f\"/*\")\n",
    "results_paths.sort()\n",
    "print(results_paths)\n",
    "models_path = glob.glob(results_paths[0] + \"/models/*\")\n",
    "# results_paths = results_paths[1]\n",
    "models_path.sort()\n",
    "# for p in models_path:\n",
    "#     print(p)\n",
    "# # print(models_path)\n",
    "# print(len(models_path))\n",
    "\n",
    "steps = 50\n",
    "# ckpt_path_idx = get_best_model(results_paths, models_path)\n",
    "ckpt_path_idx = get_best_model_idx(results_path)\n",
    "ckpt_path = results_paths[0] + f\"/models/model_epoch_{steps*(1+ckpt_path_idx)}.pth\"\n",
    "# ckpt_path = results_paths + f\"/models/model_epoch_1000.pth\"\n",
    "print(ckpt_path)\n",
    "device = TorchUtils.get_torch_device(try_to_use_cuda=True)\n",
    "\n",
    "# restore policy\n",
    "policy, ckpt_dict = FileUtils.policy_from_checkpoint(ckpt_path=ckpt_path, device=device, verbose=True)\n",
    "\n",
    "# print(os.path.exists('datasets/lift/mg/low_dim_sparse_v141_augmented_goal.hdf5'))\n",
    "# # print current directory\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Device 0 is available for rendering\n",
      "INFO:root:Device 1 is available for rendering\n",
      "INFO:root:Command '['/home/omniverse/miniforge3/envs/robomimic/lib/python3.8/site-packages/egl_probe/build/test_device', '3']' returned non-zero exit status 1.\n",
      "INFO:root:Device 3 is not available for rendering\n",
      "INFO:root:Command '['/home/omniverse/miniforge3/envs/robomimic/lib/python3.8/site-packages/egl_probe/build/test_device', '4']' returned non-zero exit status 1.\n",
      "INFO:root:Device 4 is not available for rendering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset 'object' with shape (14,)\n",
      "Loaded dataset 'robot0_eef_pos' with shape (3,)\n",
      "Loaded dataset 'robot0_eef_quat' with shape (4,)\n",
      "Loaded dataset 'robot0_eef_vel_ang' with shape (3,)\n",
      "Loaded dataset 'robot0_eef_vel_lin' with shape (3,)\n",
      "Loaded dataset 'robot0_gripper_qpos' with shape (2,)\n",
      "Loaded dataset 'robot0_gripper_qvel' with shape (2,)\n",
      "Loaded dataset 'robot0_joint_pos' with shape (7,)\n",
      "Loaded dataset 'robot0_joint_pos_cos' with shape (7,)\n",
      "Loaded dataset 'robot0_joint_pos_sin' with shape (7,)\n",
      "Loaded dataset 'robot0_joint_vel' with shape (7,)\n",
      "Created environment with name NutAssemblySquare\n",
      "Action size is 7\n",
      "============= Loaded Environment =============\n",
      "NutAssemblySquare\n",
      "{\n",
      "    \"camera_depths\": false,\n",
      "    \"camera_heights\": 84,\n",
      "    \"camera_widths\": 84,\n",
      "    \"control_freq\": 20,\n",
      "    \"controller_configs\": {\n",
      "        \"control_delta\": true,\n",
      "        \"damping\": 1,\n",
      "        \"damping_limits\": [\n",
      "            0,\n",
      "            10\n",
      "        ],\n",
      "        \"impedance_mode\": \"fixed\",\n",
      "        \"input_max\": 1,\n",
      "        \"input_min\": -1,\n",
      "        \"interpolation\": null,\n",
      "        \"kp\": 150,\n",
      "        \"kp_limits\": [\n",
      "            0,\n",
      "            300\n",
      "        ],\n",
      "        \"orientation_limits\": null,\n",
      "        \"output_max\": [\n",
      "            0.05,\n",
      "            0.05,\n",
      "            0.05,\n",
      "            0.5,\n",
      "            0.5,\n",
      "            0.5\n",
      "        ],\n",
      "        \"output_min\": [\n",
      "            -0.05,\n",
      "            -0.05,\n",
      "            -0.05,\n",
      "            -0.5,\n",
      "            -0.5,\n",
      "            -0.5\n",
      "        ],\n",
      "        \"position_limits\": null,\n",
      "        \"ramp_ratio\": 0.2,\n",
      "        \"type\": \"OSC_POSE\",\n",
      "        \"uncouple_pos_ori\": true\n",
      "    },\n",
      "    \"goal_path\": \"datasets/square/ph/low_dim_v141_augmented_goal.hdf5\",\n",
      "    \"has_offscreen_renderer\": true,\n",
      "    \"has_renderer\": false,\n",
      "    \"ignore_done\": true,\n",
      "    \"render_gpu_device_id\": 0,\n",
      "    \"reward_shaping\": false,\n",
      "    \"robots\": [\n",
      "        \"Panda\"\n",
      "    ],\n",
      "    \"use_camera_obs\": false,\n",
      "    \"use_object_obs\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# create environment from saved checkpoint\n",
    "env, _ = FileUtils.env_from_checkpoint(\n",
    "    ckpt_dict=ckpt_dict, \n",
    "    render=False, # we won't do on-screen rendering in the notebook\n",
    "    render_offscreen=True, # render to RGB images for video\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Return': 1.0, 'Horizon': 181, 'Success_Rate': 1.0}\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "rollout_horizon = 500\n",
    "if task == \"transport\":\n",
    "    rollout_horizon = 1000\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "video_path = \"rollout.mp4\"\n",
    "video_writer = imageio.get_writer(video_path, fps=20)\n",
    "\n",
    "stats = rollout(\n",
    "    policy=policy, \n",
    "    env=env, \n",
    "    horizon=rollout_horizon, \n",
    "    render=False, \n",
    "    video_writer=video_writer, \n",
    "    video_skip=1, \n",
    "    camera_names=[\"agentview\"]\n",
    ")\n",
    "print(stats)\n",
    "video_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"rollout.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robomimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
